# -*- coding: utf-8 -*-
"""EDA,РАЗВЕДОЧНЫЙ И ПЕРВИЧНЫЙ АНАЛИЗ ДАННЫХ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1utsMhGWwTy7Go1QrekVImB6FWo8lMCDR
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind
from scipy.stats import norm
from tqdm.auto import tqdm
from statsmodels.stats.power import tt_ind_solve_power
import datetime as dt
import matplotlib.pyplot as plt
import seaborn as sns
import random
from random import shuffle
import scipy.stats as stats
import pylab
import scipy.stats as stats
import statsmodels.stats.api as sms
from tqdm.notebook import tqdm
from scipy.stats import ttest_ind
from statsmodels.stats.proportion import proportions_ztest
import plotly.express as px
import plotly.graph_objects as go
import plotly
!pip install circlify
import circlify
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.cluster.hierarchy import ward, fcluster
from sklearn.cluster import KMeans
!pip install bertopic
from bertopic import BERTopic
! pip install phik
import phik
from phik.report import plot_correlation_matrix
from phik import report
!pip install --upgrade nbformat nbconvert

"""Разведочный анализ данных (Exploratory Data Analysis, EDA) — это процесс исследования и анализа данных с целью извлечения полезной информации и обнаружения закономерностей и трендов, которые могут быть использованы в дальнейшем исследовании.

EDA включает в себя исследование структуры данных, проверку наличия пропущенных значений, анализ распределения данных, определение выбросов, корреляций и зависимостей между переменными.

В рамках исследования необходимо:
исследовать данные

*   исследовать данные

*   выполнить сегментацию по пользователям и найти инсайты

*   выделить категории товаров и найти инсайты
"""

#download_data
! gdown --id 1mJjepHoxV3f3eDxjxSvCvxvcibBS2qpx
online_reatail_data = pd.read_excel('/content/online_retail_II.xlsx')
online_reatail_data.head()

# data information
online_reatail_data.info()

"""*в сете отстутствуют NaN значения*"""

# check missing values
online_reatail_data.isna().sum()

"""#Исследовательский анализ данных

Изучение истории покупок

Общие метрики: общее количество покупателей, общее количество товаров, среднее
количество заказов на одного покупателя, средний чек, среднее количество единиц товара при одной покупке.
"""

display(online_reatail_data.describe().T)
display(online_reatail_data.describe(include = ['object']).T)

"""Минимальное количество купленных товаров одним клиентом минус 9600 шт., а максимальное - 191512. Минимальная цена за товар минус 53594., максимальная - 25 111. Наибольшей популярностью пользуется товар "WHITE HANGING HEART T-LIGHT HOLDER", его купили 3549 раз из 4681 покупок за рассматриваемый период. Количество стран 40,большего всего покупок в UK
Видим первую проблему с данными. Цена и количество не может быть отрицательными. Удалим данные значения. Также приведм значения Description к нижнему регистру для удобства дальнешего анализа


"""

print('Общее количество уникальных покупателей: {:.0f}'.format(len(online_reatail_data['Customer ID'].unique())))
print('Общее количество уникальных товаров: {:.0f}'.format(len(online_reatail_data['Description'].unique())))
print('Количество уникальных заказов: {:.0f}'.format(len(online_reatail_data['Invoice'].unique())))
print('Количество уникальных стран: {:.0f}'.format(len(online_reatail_data['Country'].unique())))

#del negativ_numbers
online_reatail_data['Quantity'] = online_reatail_data['Quantity'].clip(lower=0)
online_reatail_data['Price'] = online_reatail_data['Price'].clip(lower=0)
#reg
online_reatail_data['Description']=online_reatail_data['Description'].str.lower()

display(online_reatail_data.describe().T)

#гистограмма распределения заказов
orders_per_buyer = online_reatail_data.groupby('Customer ID').agg({'Invoice':'count'})
plt.title('Гистограмма распределения количества заказов', size = 15)
sns.histplot(data = orders_per_buyer, x='Invoice', kde=True, bins=100, color='blue')
plt.xlabel('Количество заказов')
plt.ylabel('Количество уникальных покупателей')
plt.show()

"""Видим, что распределение количества заказов не выглядит как нормальное, поэтому будем использовать как меру медиану.

"""

print('Cреднее количество заказов на одного покупателя: {:.0f} шт.'.format(orders_per_buyer['Invoice'].median()))

"""Для анализа данных по месяцам нам необходимо выделить из даты заказа только месяц,а также посчитать итоговую сумму как цена на количество,плюс добавим год"""

online_reatail_data['total_price'] = online_reatail_data['Quantity'] * online_reatail_data['Price']
online_reatail_data['month'] = online_reatail_data['InvoiceDate'].dt.month
online_reatail_data['year'] = online_reatail_data['InvoiceDate'].dt.year

online_reatail_data

grouped_by_orders = online_reatail_data.groupby(['Customer ID', 'Invoice'])\
.agg({'InvoiceDate':'first', 'Description':'count', 'Quantity':'sum','Price':'mean', 'total_price':'sum'}).reset_index()\
.rename(columns={'date':'first_date_order', 'Description':'cnt_unique_products_in_order',
                 'Quantity':'total_cnt_goods_in_order', 'Price':'avg_product_price', 'total_price':'order_price'})

grouped_by_customers = grouped_by_orders.groupby('Customer ID')\
.agg({'Invoice':'nunique', 'cnt_unique_products_in_order':'mean', 'total_cnt_goods_in_order':'mean',
      'avg_product_price':'mean', 'order_price':['sum','mean']}).reset_index()

grouped_by_customers.columns = ['Customer ID', 'cnt_orders', 'avg_products_in_order', 'avg_goods_in_order',
                                'avg_product_price', 'total_revenue', 'avg_order_price']

print('Cредняя выручка с одного покупателя: {:.0f} у.е.'.format(grouped_by_customers['total_revenue'].median()))
print('Cредняя сумма заказа: {:.0f} у.е.'.format(grouped_by_customers['avg_order_price'].median()))
print('Cреднее число позиций товаров в заказах: {:.0f} шт.'.format(grouped_by_customers['avg_products_in_order'].median()))
print('Cреднее число единиц товаров в заказах: {:.0f} шт.'.format(grouped_by_customers['avg_goods_in_order'].median()))

grouped_by_customers.describe().T

print('Средний чек по всем покупателям: {:.0f} у.е.'\
      .format((grouped_by_customers['total_revenue']/grouped_by_customers['cnt_orders']).median()))

"""За рассматриваемый период времени в данных превалируют покупатели, совершившие от 1 до 6 заказов.Cредняя выручка с одного покупателя составляет 655, а средняя сумма заказа - 230. Средний чек по всем покупателям составляет 236. Видим,что у магазина имеется лояльные и постоянные клиенты которые могут совершать повторные покупки.


"""

fig, axs = plt.subplots(1, 2, figsize=(20, 5))
plt.suptitle('Распределение количества заказов на 1 покупателя', size=18)

#Построение гистограммы
grouped_by_customers['cnt_orders'].hist(bins=80, edgecolor='black', ax=axs[0])
axs[0].set_title('Гистограмма распределения количества заказов на 1 покупателя', size=13)
axs[0].set_xlabel('Количество покупателей')
axs[0].set_ylabel('количество заказов')
axs[0].axvline(x=grouped_by_customers['cnt_orders'].quantile(.95), c='r', linestyle='--')
axs[0].text(s='95% quantile: {:.2f}'.format(grouped_by_customers['cnt_orders'].quantile(.95)),
          x=10, y=500, size=12, color='r')

x_values = pd.Series(range(0,len(grouped_by_customers)))
sns.scatterplot(x=x_values, y=grouped_by_customers['cnt_orders'], edgecolor='black')
axs[1].set_title('Точечный график количества заказов по пользователям', size=13)
axs[1].set_xlabel('Количество покупателей')
axs[1].set_ylabel('Количество заказов')
axs[1].set_ylim(0,40)
plt.show()

"""На графиках видим наличие выбросов в распределении количества заказов на одного покупателя. В основном клиенты совершают до 16 покупок, оставльное - выбросы. 95% квантиль равен 16 покупкам."""

fig, axs = plt.subplots(1, 2, figsize=(20, 5))
plt.suptitle('Распределение суммарной выручки на 1 покупателя', size=18)

#Построение гистограммы
grouped_by_customers['total_revenue'].hist(bins=50, edgecolor='black', ax=axs[0])
axs[0].set_title('Гистограмма распределения суммарной выручки на 1 покупателя', size=13)
axs[0].set_xlabel('Суммарная выручка')
axs[0].set_ylabel('количество покупателей')
axs[0].axvline(x=grouped_by_customers['total_revenue'].quantile(.95), c='r', linestyle='--')
axs[0].text(s='95% quantile: {:.2f}'.format(grouped_by_customers['total_revenue'].quantile(.95)),
          x=30000, y=500, size=12, color='r')

# Построение боксплота
sns.boxplot(x='total_revenue', data=grouped_by_customers, ax=axs[1])
axs[1].set_title('Боксплот распределения суммарной выручки на 1 покупателя', size=13)
axs[1].set_xlabel('Суммарная выручка')
axs[1].set_ylabel('Количество покупателей')
axs[1].set_xlim(0, 60000)
plt.show()

"""

На графиках видим наличие выбросов в распределении количества суммарной выручки с одного покупателя. В основном клиенты покупают товары на сумму до 6171, остальное - выбросы. 95% квантиль равен 6171.
"""

print('90, 95 и 99 перцентили количества заказов:', np.percentile(grouped_by_customers['cnt_orders'], [90, 95, 99]))

"""

Не более 5% пользователей совершили больше 16 и не более 1% больше 36. Пусть граница аномальных значений проходит в диапазоне свыше 36 покупок.
"""

print('90, 95 и 99 перцентили количества заказов:', np.percentile(grouped_by_customers['total_revenue'], [90, 95, 99]))

"""Не более 5% пользователей совершили покупки дороже 6171. и не более 1% дороже 20121. Пусть граница аномальных значений проходит в диапазоне свыше 20121

"""

abnormal_orders_users = grouped_by_customers[grouped_by_customers['cnt_orders']>np.percentile(grouped_by_customers['cnt_orders'], 99)].index.to_list()
abnormal_revenue_users = grouped_by_customers[grouped_by_customers['total_revenue']>np.percentile(grouped_by_customers['total_revenue'], 99)].index.to_list()
abnormal_users = pd.concat([pd.Series(abnormal_orders_users), pd.Series(abnormal_revenue_users)]).to_list()

print('Количество покупателей с аномально большой выручкой: {}'.format(len(abnormal_revenue_users)))
print('Количество покупателей с аномально большим количеством заказов: {}'.format(len(abnormal_orders_users)))

#Отфильтруем датафрейм по аномальным пользователям
grouped_by_customers_filtered = grouped_by_customers.loc[~(grouped_by_customers['total_revenue'] > 20121),:]
df_filtered = online_reatail_data[online_reatail_data['Customer ID'].isin(grouped_by_customers_filtered['Customer ID'])]
ordersByUsers_filtered = grouped_by_customers[grouped_by_customers['cnt_orders'] <= 36]
df_filtered = df_filtered[df_filtered['Customer ID'].isin(ordersByUsers_filtered['Customer ID'])]

grouped_by_date = df_filtered.groupby('month').agg({'Customer ID':'nunique', 'total_price':'sum'}).reset_index()\
.rename(columns={'Customer ID':'cnt_buyers', 'total_price':'revenue'})
fig = px.line(grouped_by_date, x="month", y="revenue")
fig.update_layout(
    title='Помесячная динамика выручки',
    xaxis_title='Месяц',
    yaxis_title='Выручка')
fig.show()

fig = px.line(grouped_by_date, x="month", y="cnt_buyers")
fig.update_layout(
    title='Помесячная динамика количества покупателей',
    xaxis_title='Месяц',
    yaxis_title='Кол-во покупателей')
fig.show()

##Выручка по странам
d = online_reatail_data.groupby('Country')['total_price'].sum().reset_index().rename(columns={'total_price':'revenue'}).sort_values(by='revenue', ascending=False)
plt.figure(figsize=(15,6.5))
sns.set_style('darkgrid')
g = sns.barplot(data=d, x='Country', y='revenue', palette='viridis_r')
g.set_title('Revenue by country')
g.set_xticklabels(d['Country'], rotation=55, fontdict={'fontsize':7})
plt.show()

##Топ 10 наиболее продаваемые товары
m = online_reatail_data.groupby(['Description'])[['Quantity']].sum().reset_index().sort_values(by='Quantity', ascending=False).nlargest(10,'Quantity')
print(m)

##Топ 10 товаров приносящие большую выручку
grouped = online_reatail_data.groupby(['Description'])[['total_price']].sum().reset_index().rename(columns={'total_price':'revenue'}).sort_values(by='revenue', ascending=False).nlargest(10,'revenue')
print(grouped)
#фнукция подбора цвета
def get_color(name, number):
    pal = list(sns.color_palette(palette=name, n_colors=number).as_hex())
    return pal

pal_vi = get_color('viridis_r', len(online_reatail_data))

# compute circle positions:
circles = circlify.circlify(grouped['revenue'].tolist(),show_enclosure=False,target_enclosure=circlify.Circle(x=0, y=0, r=1))
circles.reverse()
#Нарисуем круг, упакованный кругами.
fig, ax = plt.subplots(figsize=(10, 10), facecolor='white')
ax.axis('off')
lim = max(max(abs(circle.x)+circle.r, abs(circle.y)+circle.r,) for circle in circles)
plt.xlim(-lim, lim)
plt.ylim(-lim, lim)
# Рисуем круги
for circle, label, emi, color in zip(circles, grouped['Description'], grouped['revenue'],pal_vi):
    x, y, r = circle
    ax.add_patch(plt.Circle((x, y), r, alpha=0.9, color = color))
    plt.annotate(label +'\n'+ format(emi, ","), (x,y), size=6, va='center', ha='center')
plt.xticks([])
plt.yticks([])
plt.show()

##Выручка по годам
t = online_reatail_data.groupby('year')['total_price'].sum().reset_index().rename(columns={'total_price':'revenue'})
plt.figure(figsize=(15,5))
ax=sns.barplot(x='year', y='revenue',data=t)
ax.set_title('Revenue by years')
plt.xticks(rotation=60,size=10)
plt.show()

"""**Вывод:** получили метрики: общее количество покупателей, общее количество товаров, среднее количество заказов на одного покупателя, средний чек, среднее количество единиц товара при одной покупке.Большая часть выручки приходится на 2010 год. Определили наиболее значимые страны по выручке: UK, EIRE, Netherlands, Germany, France. Определили наиболее значимые товары в выручке, и наиболее продаваемые товары. Пиковые значения по выручки приходятся на 11 месяц, также совпадают с количеством покупателей. Наблюдается рост выручки с (8 месяца) августа до ноября (11 месяца).

#Кластеризация пользователей

Выполним кластеризацию по пользователям. Для этой цели воспользуемся 2 наиболее распостраненными методами кластеризации.

Иерархическая кластеризация, как следует из названия, представляет собой алгоритм, который строит иерархию кластеров. Этот алгоритм начинает работу с того, что каждому экземпляру данных сопоставляется свой собственный кластер. Затем два ближайших кластера объединяются в один и так далее, пока не будет образован один общий кластер. Результат иерархической кластеризации может быть представлен с помощью дендрограммы.

k-средние - это метод кластерного анализа с использованием заранее заданного количества кластеров. Он требует предварительного знания ‘K’, основанный на минимизации суммарных квадратичных отклонений точек кластеров от центроидов (средних координат) этих кластеров.
"""

df_filtered

grouped_by_orders = df_filtered.groupby(['Customer ID', 'Invoice'])\
.agg({'InvoiceDate':'first', 'Description':'count', 'Quantity':'sum','Price':'mean', 'total_price':'sum'}).reset_index()\
.rename(columns={'date':'first_date_order', 'Description':'cnt_unique_products_in_order',
                 'Quantity':'total_cnt_goods_in_order', 'Price':'avg_product_price', 'total_price':'order_price'})

grouped_by_customers = grouped_by_orders.groupby('Customer ID')\
.agg({'Invoice':'nunique', 'cnt_unique_products_in_order':'mean', 'total_cnt_goods_in_order':'mean',
      'avg_product_price':'mean', 'order_price':['sum','mean']}).reset_index()

grouped_by_customers.columns = ['Customer ID', 'cnt_orders', 'avg_products_in_order', 'avg_goods_in_order',
                                'avg_product_price', 'total_revenue', 'avg_order_price']

grouped_by_customers

#выберем колонки по которым будем производить кластеризацию. Исходим,из того что лучший показатель для кластеризации пользователей это выручку которую приносят,средний чек заказа,средняя стоимость продуктов)
col=['total_revenue', 'avg_order_price', 'avg_product_price']

pd.options.mode.chained_assignment = None
grouped_by_customers[col].fillna(0, inplace=True) # заменим пропуски данных нулями, в противном случае выдаст ошибку

# матрица рассеяния и гистограммы
from pandas.plotting import scatter_matrix
scatter_matrix(grouped_by_customers[col], alpha=0.05, figsize=(10, 10));

grouped_by_customers[col].corr() # посмотрим на парные корреляции

"""Как видим связь у нас слабая.Для оценки зависимости лучше использовать phik который детектек не только линейную связь."""

phik_overview = grouped_by_customers[col].phik_matrix()

phik_overview

sns.heatmap(phik_overview)

"""Как видим Phik показывает более сильную зависимость переменных."""

# загружаем библиотеку препроцесинга данных
# эта библиотека автоматически приведен данные к нормальным значениям
from sklearn import preprocessing
dataNorm = preprocessing.MinMaxScaler().fit_transform(grouped_by_customers[col].values)

# Вычислим расстояния между каждым набором данных,
# т.е. строками массива data_for_clust
# Вычисляется евклидово расстояние (по умолчанию)
data_dist = pdist(dataNorm, 'euclidean')
# Главная функция иерархической кластеризии
# Объедение элементов в кластера и сохранение в
# специальной переменной (используется ниже для визуализации
# и выделения количества кластеров
data_linkage = linkage(data_dist, method='average')

# Метод локтя. Позволячет оценить оптимальное количество сегментов.
# Показывает сумму внутри групповых вариаций
last = data_linkage[-10:, 2]
last_rev = last[::-1]
idxs = np.arange(1, len(last) + 1)
plt.plot(idxs, last_rev)

acceleration = np.diff(last, 2)
acceleration_rev = acceleration[::-1]
plt.plot(idxs[:-2] + 1, acceleration_rev)
plt.show()
k = acceleration_rev.argmax() + 2
print("Рекомендованное количество кластеров:", k)

"""Суть метода локтя заключается в подсчете внутрикластерной дисперсии и выборе минимальной.(Максимальной с точки зрения снижения) Внутрекластерная дисперсия это растояние от каждой точки до центроиды."""

from sklearn.cluster import KMeans as sk_KMeans
from yellowbrick.cluster import KElbowVisualizer
model = sk_KMeans(random_state=0)
visualizer = KElbowVisualizer(model, k=(1,10), timings=False)
visualizer.fit(data_linkage)
visualizer.show();

#функция построения дендрограмм
def fancy_dendrogram(*args, **kwargs):
    max_d = kwargs.pop('max_d', None)
    if max_d and 'color_threshold' not in kwargs:
        kwargs['color_threshold'] = max_d
    annotate_above = kwargs.pop('annotate_above', 0)

    ddata = dendrogram(*args, **kwargs)

    if not kwargs.get('no_plot', False):
        plt.title('Hierarchical Clustering Dendrogram (truncated)')
        plt.xlabel('sample index or (cluster size)')
        plt.ylabel('distance')
        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):
            x = 0.5 * sum(i[1:3])
            y = d[1]
            if y > annotate_above:
                plt.plot(x, y, 'o', c=c)
                plt.annotate("%.3g" % y, (x, y), xytext=(0, -5),
                             textcoords='offset points',
                             va='top', ha='center')
        if max_d:
            plt.axhline(y=max_d, c='k')
    return ddata

nClust=3

#строим дендрограмму
fancy_dendrogram(
    data_linkage,
    truncate_mode='lastp',
    p=nClust,
    leaf_rotation=90.,
    leaf_font_size=12.,
    show_contracted=True,
    annotate_above=10,
)
plt.show()

"""*Исходя из дендограммы 3 кластера также выглядят оптимально.Если поставит 5 видим что растояние увеличилось между кластерами.*"""

# иерархическая кластеризация
clusters=fcluster(data_linkage, nClust, criterion='maxclust')

x=0 # Чтобы построить диаграмму в разных осях, меняйте номера столбцов
y=2 #
plt.figure(figsize=(10, 8))
plt.scatter(dataNorm[:,x], dataNorm[:,y], c=clusters, cmap='flag')
plt.xlabel(col[x])
plt.ylabel(col[y]);
plt.show()

# к оригинальным данным добавляем номер кластера
grouped_by_customers ['I']=clusters
res=grouped_by_customers.groupby('I')[col].mean()
res['Количество']=grouped_by_customers.groupby('I').size().values
res #ниже средние цифры по кластерам и количество объектов (Количество)

grouped_by_customers[grouped_by_customers['I']==3] # !!!!! меняйте номер кластера

# строим кластеризаци методом KMeans
km = KMeans(n_clusters=nClust).fit(dataNorm)

# выведем полученное распределение по кластерам
# так же номер кластера, к котрому относится строка, так как нумерация начинается с нуля, выводим добавляя 1
kmdata= km.labels_ +1
kmdata

x=0 # Чтобы построить диаграмму в разных осях, меняйте номера столбцов
y=1 #
centroids = km.cluster_centers_
plt.figure(figsize=(10, 8))
plt.scatter(dataNorm[:,x], dataNorm[:,y], c=km.labels_, cmap='flag')
plt.scatter(centroids[:, x], centroids[:, y], marker='*', s=300,
            c='b', label='centroid')
plt.xlabel(col[x])
plt.ylabel(col[y]);
plt.show()

# к оригинальным данным добавляем номера кластеров
grouped_by_customers['KMeans']=km.labels_+1
res=grouped_by_customers.groupby('KMeans')[col].mean()
res['Количество']=grouped_by_customers.groupby('KMeans').size().values
res

grouped_by_customers[grouped_by_customers['KMeans']==2] # изменяйте номер кластера, содержание которого хотите просмотреть

grouped_by_customers[grouped_by_customers['KMeans']==3][['Customer ID', 'cnt_orders', 'avg_products_in_order', 'avg_goods_in_order', 'avg_product_price', 'total_revenue',
       'avg_order_price']]

"""**Вывод**: как мы видим, три кластера, сформированные моделями KMeans и иерхарирической кластеризации, имеют примерно одинаковый размер 1 большой кластер и 2 поменьше. K-means удалось чуть лучше сегментировать. Исходя из этого, мы можем понять, что полученные сегменты клиентов более или менее одинаковы, независимо от того, какой алгоритм мы используем."""

sns.scatterplot(data=grouped_by_customers, x="total_revenue", y="avg_order_price", hue=kmdata, palette="rainbow").set_title('Clusters of customers using K-Means Clustering')
plt.show()
sns.scatterplot(data=grouped_by_customers, x="total_revenue", y="avg_order_price", hue=clusters, palette="rainbow").set_title('Clusters of customers using Hierarchical Clustering')
plt.show()

"""Сравним поведение покупателей из разных кластеров после сегментации

"""

grouped_by_customers

orders1_filt_clust = grouped_by_customers.groupby('KMeans')\
.agg({'Customer ID':'nunique', 'avg_products_in_order':'mean', 'avg_goods_in_order':'mean', 'cnt_orders':'sum', 'avg_product_price':'mean',
      'total_revenue':'sum','avg_order_price':'mean'})\
.reset_index().rename(columns={'Customer ID':'n_buyers'})

orders1_filt_clust['avg_orders_per_buyer'] = round(orders1_filt_clust['cnt_orders']/orders1_filt_clust['n_buyers'],0)
orders1_filt_clust['avg_bill'] = orders1_filt_clust['total_revenue'] / orders1_filt_clust['cnt_orders']
orders1_filt_clust['avg_goods_per_buyer'] = round(orders1_filt_clust['avg_goods_in_order'] / orders1_filt_clust['n_buyers'], 0)

WIDTH = 3
plot_amount = len(orders1_filt_clust.columns)
height = plot_amount//WIDTH

fig, axs = plt.subplots(height, WIDTH, figsize=(20, 17))
fig.suptitle('Барплоты метрик по категориям',  y=1.003, size=14)

for item, ax in zip(orders1_filt_clust.columns[1:], np.ravel(axs)):
    sns.barplot(data = orders1_filt_clust, x='KMeans', y=item, ax=ax)
    ax.set_title(item.capitalize().replace('_', ' '), size=12)

plt.tight_layout()
plt.show()

"""**Вывод**: можем наблюдать  по совокупной выручке и количеству позиций лидирует 2 кластер. При этом по среднему чеку,по по количеству заказов на покупателя,по средней цене заказа, по количеству в заказе лидирует 3 кластер. Соотвественно,компании можно рекомендовать уделять внимание 2 кластер так как он приносит больше выручки,при это количество покупателей среднее,и количество заказов. Это условно лояльные клиенты которые заказывают несколько позиций разных товаров но в небольшом количестве и с низкой средней ценой.

#Классификация товаров на категории

У нас имеються названия товаров, но нет категорий. Это можно решить с помщью моделей NLP. Например помощью библитеки BERtopic которая разделит наименование заказа на слова,а затем подсчитает наиболее часто встречаемые, которые мы уже можем использовать как категории.
"""

df_filtered

model = BERTopic(verbose=True)

#convert to list
sampl = df_filtered.iloc[0:300000].astype(str)
docs = sampl['Description'].to_list()

topics, probabilities = model.fit_transform(docs)
model.get_topic_freq()

model.visualize_barchart()

model.get_topic_info()

#исходя из топиков можно выделить следующие категории
food_and_fruits = ['strawberry', 'cake','lunch','teatime','baking']
furniture_instruments = ['door','home','building','ceramic','flight','mat']
home = ['holder','trinket','flag','cakestand']
bags = ['bag','box','pack']

#функцию, которая определит наличие ключевого слова в названии товара и отнесёт товар к одной из категорий:
def to_category(product_name):
    if any(i in product_name.lower() for i in food_and_fruits):
        return 'food_and_fruits'
    elif any(i in product_name.lower() for i in furniture_instruments):
        return 'furniture_instruments'
    elif any(i in product_name.lower() for i in home):
        return 'home'
    elif any(i in product_name.lower() for i in bags):
        return 'bags'
    else:
        return 'other'

#добавим категорию в данные
sampl['product_category'] = sampl['Description'].apply(to_category)

#количество заказов каждой категории
categories_amount = sampl.groupby(['Description','product_category'])[['Invoice']].nunique().reset_index()\
.groupby('product_category', as_index=False)['Description'].count().sort_values('Description', ascending=False)
categories_amount['percent_of_products'] = (categories_amount['Description']/categories_amount['Description'].sum()).map('{:.2%}'.format)
categories_amount

#выручка и % по категориям
sampl['total_price'] = pd.to_numeric(sampl['total_price'], errors='coerce').fillna(0).astype(np.int64)
categories_revenue = sampl.groupby('product_category')['total_price'].sum().reset_index().rename(columns={'total_price':'revenue'})\
.groupby('product_category', as_index=False)['revenue'].sum().sort_values('product_category', ascending=False)
categories_revenue['percent_of_products'] = (categories_revenue['revenue']/categories_revenue['revenue'].sum()).map('{:.2%}'.format)
categories_revenue

categories_amount.plot(x='product_category', y='Description', kind='bar', figsize=(12, 3), legend=False, alpha=.5)
plt.title("Количество наименований товаров в зависимости категории")
plt.xlabel("Категория")
plt.ylabel("Количество наименований")
plt.xticks(rotation=-30, ha='left')
plt.show()

categories_revenue.plot(x='product_category', y='revenue', kind='bar', figsize=(12, 3), legend=False, alpha=.5)
plt.title("выручка в зависимости категории")
plt.xlabel("Категория")
plt.ylabel("Выручка")
plt.xticks(rotation=-30, ha='left')
plt.show()

"""*Выделение сегментов заказов прошло не очень удачно более половины всех заказов относятся к категории others*

Анализ категорий от времени
"""

sampl['InvoiceDate'] = pd.to_datetime(sampl['InvoiceDate'])

# group by "X" column
groups = sampl.groupby('product_category')

# extract keys from groups
keys = sampl['product_category'].unique()
keys = keys[keys != 'other'] #уберем категорию others

for i in keys:
    sampl_for_merge_product = groups.get_group(i)
    series_mean = sampl_for_merge_product[['InvoiceDate','total_price']].groupby(pd.Grouper(key ='InvoiceDate',freq = 'M')).mean()
    series_std = sampl_for_merge_product[['InvoiceDate','total_price']].groupby(pd.Grouper(key ='InvoiceDate',freq = 'M')).std().fillna(0)
    f, ax = plt.subplots(1, 1, figsize = (12, 4))
    ax.plot(series_mean, linewidth=4, color = 'black')
    ax.fill_between(series_mean.index,
               (series_mean.values-series_std.values).ravel(),
               (series_mean.values+series_std.values).ravel(),
              color = 'cadetblue',alpha=1)
    ax.set_title(f'Mean revenue "{i}"')
    ax.set_xlabel('month')

"""**Вывод**:
Стандартное отклонение важно, потому что оно говорит нам, насколько разбросаны значения в данном наборе данных.
В результате можем выделить некоторые особенности стандрных отклонения выручек сегментов. У таких сегментов как furniture_instruments,food and fruits несколько периодов повышенной волотильности. Можно предположить что в данные месяца товар пользуется высоким спросом и выручка становиться выше средней. Интересный кейс у категории bags повышенная волотильность выруки наблюдается к концу года,что возможно связано с активностью покупателей в связи с новогодним праздникам. У всех категорий кроме bags повышенная волотильность в летние месяцы (6-7 месяца).

Сравним поведение покупателей из разных кластеров после сегментации
"""

df_seg_copy = sampl.copy()
index = df_seg_copy[df_seg_copy['product_category'] == 'other'].index
df_seg_copy.drop(index, inplace=True)
df_seg_copy

"""Сгруппируем получившуюся таблицу по кластерам и найдем метрики: среднее количество заказов, среднее количество купленных товаров на одного покупателя, средний чек, суммарную выручку."""

df_seg_copy['Price'] = pd.to_numeric(df_seg_copy['Price'], errors='coerce').fillna(0).astype(np.int64)
df_seg_copy['Quantity'] = pd.to_numeric(df_seg_copy['Quantity'], errors='coerce').fillna(0).astype(np.int64)
orders_filt_clust = df_seg_copy.groupby('product_category')\
.agg({'Customer ID':'nunique', 'Invoice':'nunique', 'Description':'count', 'Quantity':'sum', 'Price':'mean',
      'total_price':'sum'})\
.reset_index().rename(columns={'Customer ID':'n_buyers', 'Invoice':'cnt_orders', 'Description':'cnt_products_in_order',
                               'Quantity':'cnt_goods_in_order','Price':'avg_price','total_price':'revenue'})

orders_filt_clust['avg_orders_per_buyer'] = round(orders_filt_clust['cnt_orders']/orders_filt_clust['n_buyers'],0)
orders_filt_clust['avg_bill'] = orders_filt_clust['revenue'] / orders_filt_clust['cnt_orders']
orders_filt_clust['avg_goods_per_buyer'] = round(orders_filt_clust['cnt_goods_in_order'] / orders_filt_clust['n_buyers'], 0)

WIDTH = 3
plot_amount = len(orders_filt_clust.columns)
height = plot_amount//WIDTH

fig, axs = plt.subplots(height, WIDTH, figsize=(20, 17))
fig.suptitle('Барплоты метрик по категориям',  y=1.003, size=14)

for item, ax in zip(orders_filt_clust.columns[1:], np.ravel(axs)):
    sns.barplot(data = orders_filt_clust, x='product_category', y=item, ax=ax)
    ax.set_title(item.capitalize().replace('_', ' '), size=12)

plt.tight_layout()
plt.show()

"""**Вывод**: исходя из данных можно выделить 2 основных сегмента bags и food-and_fruits,которые лидируют по большинству показателей: кол-во покупателей, кол-во заказов,количество продуктов в заказах, выручка, средний чек, среднее количеством товаров на покупателя. Соответственно за данными сегментами компаниии нужно следить."""